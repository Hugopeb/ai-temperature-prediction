{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13e7ad4",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "\n",
    "Let’s now move on to building a functional neural network capable of forecasting temperature. To do this, we first need to choose the most suitable tool for our task. There are several powerful open-source libraries available for developing neural networks, such as Google’s JAX or TensorFlow, and Meta’s PyTorch—all well-equipped for a project of this scope. However, since work with PyTorch had already begun prior to my arrival, we decided to continue using it. PyTorch is a well-established library with extensive documentation and community support. It's also known for being intuitive, supports easy integration with CUDA for faster training, and follows an object-oriented approach where most components are implemented as classes. We will now dive into the core of our project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caa16bb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m \n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import torch\n",
    "import joblib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from datetime import datetime\n",
    "\n",
    "print('Librerías importadas')\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(timestamp)\n",
    "\n",
    "hidden1 = 128\n",
    "hidden2 = 64\n",
    "hidden3 = 32\n",
    "layers = [hidden1, hidden2, hidden3]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "dropout = 0.2\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "run_suffix = f\"{hidden1}_{hidden2}_{hidden3}_{dropout}_{batch_size}_{learning_rate}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5109abe8",
   "metadata": {},
   "source": [
    "Above, we defined the key parameters of our neural network. It consists of three hidden layers with 128, 64, and 32 neurons, respectively. We also set the learning rate, a crucial parameter that controls how quickly the model updates its weights and how effectively it navigates the cost function landscape.\n",
    "\n",
    "To prevent the network from overfitting or relying too heavily on a few neurons, we included dropout. Additionally, we specified the batch size, which determines how many data samples are processed together before updating the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a3639",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv('WRFTA_v1B.csv')\n",
    "ds['fecha'] = pd.to_datetime(ds['fecha'])\n",
    "\n",
    "f = ds.drop(columns = ['fecha', 'estacion', 'QSNOW_0','QSNOW_7','QSNOW_12','hora','dia','z_score','mean','std'])\n",
    "df = df.dropna()\n",
    "print(df.columns)\n",
    "\n",
    "X = df.drop(columns =['TA','peso'])\n",
    "Y = df['TA'] \n",
    "weights = df['peso']\n",
    "\n",
    "# We create a directory to save the model and artifacts\n",
    "run_name = f\"WRFTA_{run_suffix}\"\n",
    "run_dir = f\"./Modelos/{run_name}/{timestamp}\"\n",
    "os.makedirs(run_dir, exist_ok=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562806e",
   "metadata": {},
   "source": [
    "Given that the initial DataFrame contained highly correlated variables, particularly between measurements of the same variable at different altitudes, we chose to work with a new dataset. The new variables are taken from altitude levels that are significantly farther apart, helping to ensure low correlation between them. Below is a brief explanation of the variables used in our dataset:\n",
    "\n",
    "General Metadata\n",
    "\n",
    "fecha: Date of the observation.\n",
    "\n",
    "estacion: Identifier of the weather station.\n",
    "\n",
    "x, y: Grid coordinates in the model domain.\n",
    "\n",
    "hora, dia: Hour and day of the observation.\n",
    "\n",
    "sin_hora, cos_hora, sin_dia, cos_dia: Sine and cosine transformations of hour and day, used to capture cyclical patterns in time.\n",
    "\n",
    "porcentaje_mar: Percentage of the grid cell covered by sea.\n",
    "\n",
    "altura: Elevation (altitude) of the location.\n",
    "\n",
    "Geolocation\n",
    "\n",
    "XLAT: Latitude of the observation point.\n",
    "\n",
    "XLONG: Longitude of the observation point.\n",
    "\n",
    "Surface-Level Meteorological Variables\n",
    "PSFC: Surface pressure.\n",
    "\n",
    "T2: Air temperature at 2 meters above the ground.\n",
    "\n",
    "PREC: Precipitation amount.\n",
    "\n",
    "Atmospheric Variables at Different Altitudes\n",
    "These variables are measured at three different vertical levels: level 0 (near the surface), level 7 (mid-atmosphere), and level 12 (higher altitude).\n",
    "\n",
    "Each level includes the following variables:\n",
    "\n",
    "z_[level]: Height of the atmospheric level.\n",
    "\n",
    "QVAPOR_[level]: Water vapor mixing ratio.\n",
    "\n",
    "QRAIN_[level]: Rainwater mixing ratio.\n",
    "\n",
    "QSNOW_[level]: Snow mixing ratio.\n",
    "\n",
    "QGRAUP[level]: Graupel (ice pellet) mixing ratio.\n",
    "\n",
    "U_[level], V_[level]: Wind components in the east-west (U) and north-south (V) directions.\n",
    "\n",
    "T_[level]: Temperature at the given level.\n",
    "\n",
    "Target Variable\n",
    "TA: The target temperature we aim to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68117d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "# Escalado de salida (Y)S\n",
    "scaler_Y = StandardScaler()\n",
    "Y_scaled = scaler_Y.fit_transform(Y.values.reshape(-1, 1)).flatten()\n",
    "# Es necesario guardar los reescalados para después cargarlos en el código modelo_entrenado.py y poder reescalar así los valores finales.\n",
    "joblib.dump(scaler_X, os.path.join(run_dir, 'scaler_XWRF.save'))\n",
    "joblib.dump(scaler_Y, os.path.join(run_dir, 'scaler_YWRF.save'))\n",
    "\n",
    "# Separamos los datos en conjuntos de entrenamiento y de testeo diferentes para ver si realmente está aprendiendo la red.\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_scaled, Y_scaled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "np.save(os.path.join(run_dir,'featuresWRF.npy'), X_scaled)\n",
    "np.save(os.path.join(run_dir,'targetsWRF.npy'), Y_scaled)\n",
    "\n",
    "np.save(os.path.join(run_dir,'features_trainWRF.npy'), X_train)\n",
    "np.save(os.path.join(run_dir,'targets_trainWRF.npy'), Y_train)\n",
    "\n",
    "np.save(os.path.join(run_dir,'features_valWRF.npy'), X_val)\n",
    "np.save(os.path.join(run_dir,'targets_valWRF.npy'), Y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8ef190",
   "metadata": {},
   "source": [
    "We normalize the data using the StandardScaler() function, which standardizes each feature by removing the mean and scaling to unit variance. In other words, it subtracts the mean and divides by the standard deviation for each column.\n",
    "\n",
    "After normalization, the dataset is split into two subsets:\n",
    "\n",
    "80% for training the model\n",
    "\n",
    "20% for validating its performance\n",
    "\n",
    "This approach is common in neural network training, as it helps ensure the model is not simply memorizing the data but is genuinely learning to make accurate predictions—in our case, predicting temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d1df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class DATA(Dataset):\n",
    "    def __init__(self, path_x, path_y):\n",
    "        self.features = np.load(path_x) \n",
    "        self.targets = np.load(path_y) \n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.features) \n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        x = torch.from_numpy(self.features[idx].copy()).float()\n",
    "        y = torch.tensor(self.targets[idx], dtype=torch.float32) \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb97efe",
   "metadata": {},
   "source": [
    "In this section, we define a custom PyTorch dataset class called DATA, which inherits from torch.utils.data.Dataset. This allows PyTorch to treat our data as a dataset.\n",
    "\n",
    "__init__: Loads features and targets from .npy files using NumPy.\n",
    "\n",
    "__len__: Returns the number of samples in the dataset.\n",
    "\n",
    "__getitem__: Returns one sample (input and target) as PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00502af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DATA(\n",
    "    os.path.join(run_dir, 'features_trainWRF.npy'), \n",
    "    os.path.join(run_dir, 'targets_trainWRF.npy'),\n",
    "    )\n",
    "    \n",
    "val_dataset = DATA(\n",
    "    os.path.join(run_dir, 'features_valWRF.npy'), \n",
    "    os.path.join(run_dir, 'targets_valWRF.npy'),\n",
    "    )\n",
    "\n",
    "dataset = DATA(\n",
    "    os.path.join(run_dir, 'featuresWRF.npy'), \n",
    "    os.path.join(run_dir, 'targetsWRF.npy'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ddab3e",
   "metadata": {},
   "source": [
    "We then create three different DATA instances assigning its features and targets paths. This allows PyTorch to treat our data as a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650132dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True) \n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n",
    "loader = DataLoader(dataset, batch_size = batch_size, shuffle = False) \n",
    "\n",
    "total_batches = len(train_loader) \n",
    "print(f'Número de batches: {total_batches}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c32200",
   "metadata": {},
   "source": [
    "Lastly, we use DataLoader to efficiently load data in batches: \n",
    "\n",
    "train_loader: Loads training data in shuffled batches.\n",
    "\n",
    "val_loader: Loads validation data without shuffling.\n",
    "\n",
    "loader: Loads the full dataset, also without shuffling.\n",
    "\n",
    "## Neural network\n",
    "\n",
    "In this section, we define a fully connected neural network using PyTorch by creating a class that inherits from nn.Module. The model takes the number of input features (input_size) as a parameter and is composed of three hidden layers with ReLU activation functions and dropout layers in between. These dropout layers help prevent overfitting by randomly disabling a fraction of neurons during training. The architecture is built using nn.Sequential, which stacks the layers in the order they are applied.\n",
    "\n",
    "Each hidden layer reduces the dimensionality of the data and learns more abstract representations. The final layer consists of a single neuron that outputs the predicted temperature given an initial data array. The forward() method defines how the input tensor flows through the network and returns the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2939c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RedNeuronal(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RedNeuronal, self).__init__()\n",
    "        self.red = nn.Sequential( \n",
    "            nn.Linear(input_size,hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden1,hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden2,hidden3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden3,1),  # hidden 3\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.red(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dddc4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_scaled.shape[1] \n",
    "print(f'input_size: {input_size}')\n",
    "\n",
    "model = RedNeuronal(input_size)\n",
    "criterion = torch.nn.MSELoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03828f61",
   "metadata": {},
   "source": [
    "Once the model is defined, we extract the input size (i.e., the number of features) directly from the DataFrame to configure the input layer accordingly. After that, we set up three key components from PyTorch that are essential for training: the loss function, the optimizer, and the learning rate scheduler.\n",
    "\n",
    "criterion defines the loss function that measures how well the model performs. In our case, we use Mean Squared Error (MSE), which is commonly used for regression tasks.\n",
    "\n",
    "optimizer is the algorithm that adjusts the model’s weights to minimize the loss. We use a built-in optimizer from PyTorch (Adam), which updates the weights to improve temperature predictions over time.\n",
    "\n",
    "scheduler dynamically adjusts the learning rate during training. Since our model has a large number of parameters (e.g., weights and biases across layers of sizes 128, 64, and 32), the cost function’s surface is complex with many valleys. Initially, the optimizer takes larger steps to explore the space, but as it approaches a minimum, the scheduler reduces the learning rate (by a factor of 0.5 after 3 stagnant epochs in our case). This strategy helps the model converge more precisely to an optimal solution without overshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83fe024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.001):\n",
    "        self.patience = patience            \n",
    "        self.min_delta = min_delta          \n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0  # Se reinicia porque hay mejora\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "early_stopper = EarlyStopping(patience=10, min_delta=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b48e6",
   "metadata": {},
   "source": [
    "The final class we define is EarlyStopping, an essential component for training our neural network efficiently. Its purpose is to stop the training process if the model does not show meaningful improvement after a certain number of consecutive epochs. Specifically, if the validation loss does not improve by at least min_delta = 0.001 over 10 consecutive epochs, training will be halted. This helps prevent overfitting and saves computational resources by avoiding unnecessary training once the model has stopped learning and achieved a locally optimal configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06791460",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 60\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (x_batch, y_batch, weights_batch) in enumerate(train_loader):  # cada batch\n",
    "        # Step 1: forward \n",
    "        pred = model(x_batch).squeeze()  \n",
    "\n",
    "        # Step 2: error\n",
    "        losses = criterion(pred, y_batch)\n",
    "        loss = losses.mean()\n",
    "\n",
    "        # Step 3: backward \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Step 4: update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step 5: accumulate loss\n",
    "        epoch_loss += loss.item()  \n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)  \n",
    "    model.eval()  \n",
    "    val_loss = 0\n",
    "    preds_celsius = []\n",
    "    targets_celsius = []\n",
    "\n",
    "    # End of the training loop. Validation begins\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            pred_val = model(x_val).squeeze()\n",
    "            per_sample_loss = criterion(pred_val, y_val)\n",
    "            loss = per_sample_loss.mean()  # e\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            pred_kelvin = scaler_Y.inverse_transform(pred_val.cpu().numpy().reshape(-1, 1))\n",
    "            target_kelvin = scaler_Y.inverse_transform(y_val.cpu().numpy().reshape(-1, 1))\n",
    "            pred_c = pred_kelvin - 273.15\n",
    "            target_c = target_kelvin - 273.15\n",
    "\n",
    "            preds_celsius.extend(pred_c.flatten())\n",
    "            targets_celsius.extend(target_c.flatten())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    MAE_celsius = np.mean(np.abs(np.array(preds_celsius)-np.array(targets_celsius))) # Error  \n",
    "\n",
    "    # Saving the best model \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_path = f\"./Modelos/{run_name}/{timestamp}/WRFTA_{run_suffix}.pt\"\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    # Scheduler checks the average loss\n",
    "    scheduler.step(avg_val_loss) \n",
    "    # Early stopper checks the average loss\n",
    "    early_stopper(avg_val_loss) \n",
    "\n",
    "    model.train()\n",
    "    # Printing the metrics for each epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss entrenamiento: {avg_loss:.4f}, Loss validación (escalado): {avg_val_loss:.4f}, MAE validación (°C): {MAE_celsius:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopper.early_stop:\n",
    "        print(f\"Parada temprana en la época {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Loading the best model\n",
    "print('Cargando el mejor modelo guardado...')\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "print('Comenzando la predicción...')\n",
    "model.eval()\n",
    "all_preds_scaled = []\n",
    "all_targets_scaled = []\n",
    "\n",
    "# We use the best model to make predictions on the entire dataset\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in loader:\n",
    "        y_pred = model(x_batch).squeeze()\n",
    "        all_preds_scaled.append(y_pred.numpy())\n",
    "        all_targets_scaled.append(y_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_scaled = np.concatenate(all_preds_scaled).reshape(-1, 1)\n",
    "all_targets_scaled = np.concatenate(all_targets_scaled).reshape(-1, 1)\n",
    "\n",
    "# Desescalamos\n",
    "y_pred_real = scaler_Y.inverse_transform(all_preds_scaled).flatten()\n",
    "y_test_real = scaler_Y.inverse_transform(all_targets_scaled).flatten()\n",
    "\n",
    "# DataFrame con resultados completos\n",
    "df_resultados = pd.DataFrame({\n",
    "    'y_pred_C': y_pred_real - 273.15, # OJO y_pred_real es la predicción del modelo\n",
    "    'TA_C': y_test_real - 273.15 # y_test_real es la temperatura observada\n",
    "})\n",
    "\n",
    "print(f'Error medio absoluto (MAE): {(np.mean(np.abs(y_test_real - y_pred_real))):.4f}')\n",
    "\n",
    "print(f'ds.columns: {ds.columns}')\n",
    "print(f'ds.shape {ds.shape}')\n",
    "print(f'df_resultados.columns: {df_resultados.columns}')\n",
    "print(f'df_resultado.shape: {df_resultados.shape}')\n",
    "\n",
    "# Pegamos directamente las predicciones al dataframe original\n",
    "ds = ds.reset_index(drop=True)  # Aseguramos índices alineados\n",
    "\n",
    "# Añadimos la columna de predicciones al df original\n",
    "df = pd.concat([ds[['fecha']], df_resultados], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a5d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VENV",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
