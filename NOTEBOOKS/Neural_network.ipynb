{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13e7ad4",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "\n",
    "Let’s now move on to building a functional neural network capable of forecasting temperature. To do this, we first need to choose the most suitable tool for our task. There are several powerful open-source libraries available for developing neural networks, such as Google’s JAX or TensorFlow, and Meta’s PyTorch—all well-equipped for a project of this scope. However, since work with PyTorch had already begun prior to my arrival, we decided to continue using it. PyTorch is a well-established library with extensive documentation and community support. It's also known for being intuitive, supports easy integration with CUDA for faster training, and follows an object-oriented approach where most components are implemented as classes. We will now dive into the core of our project.\n",
    "\n",
    "## Data management in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa16bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías importadas\n",
      "20250812_220531\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import torch\n",
    "import joblib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print('Librerías importadas')\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(timestamp)\n",
    "\n",
    "hidden1 = 128\n",
    "hidden2 = 64\n",
    "hidden3 = 32\n",
    "layers = [hidden1, hidden2, hidden3]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "dropout = 0.2\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "run_suffix = f\"{hidden1}_{hidden2}_{hidden3}_{dropout}_{batch_size}_{learning_rate}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5109abe8",
   "metadata": {},
   "source": [
    "Above, we defined the key parameters of our neural network. It consists of three hidden layers with 128, 64, and 32 neurons, respectively. We also set the learning rate, a crucial parameter that controls how quickly the model updates its weights and how effectively it navigates the cost function landscape.\n",
    "\n",
    "To prevent the network from overfitting or relying too heavily on a few neurons, we included dropout. Additionally, we specified the batch size, which determines how many data samples are processed together before updating the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a3639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hugo/GitHub/ai-temperature-prediction/NOTEBOOKS\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "ds = pd.read_csv('../../../WRFTA_v1B.csv')\n",
    "ds['fecha'] = pd.to_datetime(ds['fecha'])\n",
    "\n",
    "f = ds.drop(columns = ['fecha', 'estacion', 'QSNOW_0','QSNOW_7','QSNOW_12','hora','dia'])\n",
    "df = ds.dropna()\n",
    "print(df.columns)\n",
    "\n",
    "X = df.drop(columns =['TA','peso'])\n",
    "Y = df['TA'] \n",
    "weights = df['peso']\n",
    "\n",
    "# We create a directory to save the model and artifacts\n",
    "run_name = f\"WRFTA_{run_suffix}\"\n",
    "run_dir = f\"./Modelos/{run_name}/{timestamp}\"\n",
    "os.makedirs(run_dir, exist_ok=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562806e",
   "metadata": {},
   "source": [
    "Given that the initial DataFrame contained highly correlated variables, particularly between measurements of the same variable at different altitudes, we chose to work with a new dataset. The new variables are taken from altitude levels that are significantly farther apart, helping to ensure low correlation between them. Below is a brief explanation of the variables used in our dataset:\n",
    "\n",
    "General Metadata\n",
    "\n",
    "fecha: Date of the observation.\n",
    "\n",
    "estacion: Identifier of the weather station.\n",
    "\n",
    "x, y: Grid coordinates in the model domain.\n",
    "\n",
    "hora, dia: Hour and day of the observation.\n",
    "\n",
    "sin_hora, cos_hora, sin_dia, cos_dia: Sine and cosine transformations of hour and day, used to capture cyclical patterns in time.\n",
    "\n",
    "porcentaje_mar: Percentage of the grid cell covered by sea.\n",
    "\n",
    "altura: Elevation (altitude) of the location.\n",
    "\n",
    "Geolocation\n",
    "\n",
    "XLAT: Latitude of the observation point.\n",
    "\n",
    "XLONG: Longitude of the observation point.\n",
    "\n",
    "Surface-Level Meteorological Variables\n",
    "PSFC: Surface pressure.\n",
    "\n",
    "T2: Air temperature at 2 meters above the ground.\n",
    "\n",
    "PREC: Precipitation amount.\n",
    "\n",
    "Atmospheric Variables at Different Altitudes\n",
    "These variables are measured at three different vertical levels: level 0 (near the surface), level 7 (mid-atmosphere), and level 12 (higher altitude).\n",
    "\n",
    "Each level includes the following variables:\n",
    "\n",
    "z_[level]: Height of the atmospheric level.\n",
    "\n",
    "QVAPOR_[level]: Water vapor mixing ratio.\n",
    "\n",
    "QRAIN_[level]: Rainwater mixing ratio.\n",
    "\n",
    "QSNOW_[level]: Snow mixing ratio.\n",
    "\n",
    "QGRAUP[level]: Graupel (ice pellet) mixing ratio.\n",
    "\n",
    "U_[level], V_[level]: Wind components in the east-west (U) and north-south (V) directions.\n",
    "\n",
    "T_[level]: Temperature at the given level.\n",
    "\n",
    "Target Variable\n",
    "TA: The target temperature we aim to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68117d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "# Escalado de salida (Y)S\n",
    "scaler_Y = StandardScaler()\n",
    "Y_scaled = scaler_Y.fit_transform(Y.values.reshape(-1, 1)).flatten()\n",
    "# Es necesario guardar los reescalados para después cargarlos en el código modelo_entrenado.py y poder reescalar así los valores finales.\n",
    "joblib.dump(scaler_X, os.path.join(run_dir, 'scaler_XWRF.save'))\n",
    "joblib.dump(scaler_Y, os.path.join(run_dir, 'scaler_YWRF.save'))\n",
    "\n",
    "# Separamos los datos en conjuntos de entrenamiento y de testeo diferentes para ver si realmente está aprendiendo la red.\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_scaled, Y_scaled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "np.save(os.path.join(run_dir,'featuresWRF.npy'), X_scaled)\n",
    "np.save(os.path.join(run_dir,'targetsWRF.npy'), Y_scaled)\n",
    "\n",
    "np.save(os.path.join(run_dir,'features_trainWRF.npy'), X_train)\n",
    "np.save(os.path.join(run_dir,'targets_trainWRF.npy'), Y_train)\n",
    "\n",
    "np.save(os.path.join(run_dir,'features_valWRF.npy'), X_val)\n",
    "np.save(os.path.join(run_dir,'targets_valWRF.npy'), Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8ef190",
   "metadata": {},
   "source": [
    "We normalize the data using the StandardScaler() function, which standardizes each feature by removing the mean and scaling to unit variance. In other words, it subtracts the mean and divides by the standard deviation for each column.\n",
    "\n",
    "After normalization, the dataset is split into two subsets:\n",
    "\n",
    "80% for training the model\n",
    "\n",
    "20% for validating its performance\n",
    "\n",
    "This approach is common in neural network training, as it helps ensure the model is not simply memorizing the data but is genuinely learning to make accurate predictions—in our case, predicting temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d1df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class DATA(Dataset):\n",
    "    def __init__(self, path_x, path_y):\n",
    "        self.features = np.load(path_x) \n",
    "        self.targets = np.load(path_y) \n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.features) \n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        x = torch.from_numpy(self.features[idx].copy()).float()\n",
    "        y = torch.tensor(self.targets[idx], dtype=torch.float32) \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb97efe",
   "metadata": {},
   "source": [
    "In this section, we define a custom PyTorch dataset class called DATA, which inherits from torch.utils.data.Dataset. This allows PyTorch to treat our data as a dataset.\n",
    "\n",
    "__init__: Loads features and targets from .npy files using NumPy.\n",
    "\n",
    "__len__: Returns the number of samples in the dataset.\n",
    "\n",
    "__getitem__: Returns one sample (input and target) as PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00502af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DATA(\n",
    "    os.path.join(run_dir, 'features_trainWRF.npy'), \n",
    "    os.path.join(run_dir, 'targets_trainWRF.npy'),\n",
    "    )\n",
    "    \n",
    "val_dataset = DATA(\n",
    "    os.path.join(run_dir, 'features_valWRF.npy'), \n",
    "    os.path.join(run_dir, 'targets_valWRF.npy'),\n",
    "    )\n",
    "\n",
    "dataset = DATA(\n",
    "    os.path.join(run_dir, 'featuresWRF.npy'), \n",
    "    os.path.join(run_dir, 'targetsWRF.npy'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ddab3e",
   "metadata": {},
   "source": [
    "We then create three different DATA instances assigning its features and targets paths. This allows PyTorch to treat our data as a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650132dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True) \n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n",
    "loader = DataLoader(dataset, batch_size = batch_size, shuffle = False) \n",
    "\n",
    "total_batches = len(train_loader) \n",
    "print(f'Número de batches: {total_batches}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c32200",
   "metadata": {},
   "source": [
    "Lastly, we use DataLoader to efficiently load data in batches: \n",
    "\n",
    "train_loader: Loads training data in shuffled batches.\n",
    "\n",
    "val_loader: Loads validation data without shuffling.\n",
    "\n",
    "loader: Loads the full dataset, also without shuffling.\n",
    "\n",
    "## Neural network\n",
    "\n",
    "In this section, we define a fully connected neural network using PyTorch by creating a class that inherits from nn.Module. The model takes the number of input features (input_size) as a parameter and is composed of three hidden layers with ReLU activation functions and dropout layers in between. These dropout layers help prevent overfitting by randomly disabling a fraction of neurons during training. The architecture is built using nn.Sequential, which stacks the layers in the order they are applied.\n",
    "\n",
    "Each hidden layer reduces the dimensionality of the data and learns more abstract representations. The final layer consists of a single neuron that outputs the predicted temperature given an initial data array. The forward() method defines how the input tensor flows through the network and returns the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2939c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RedNeuronal(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RedNeuronal, self).__init__()\n",
    "        self.red = nn.Sequential( \n",
    "            nn.Linear(input_size,hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden1,hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden2,hidden3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden3,1),  # hidden 3\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.red(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dddc4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_scaled.shape[1] \n",
    "print(f'input_size: {input_size}')\n",
    "\n",
    "model = RedNeuronal(input_size)\n",
    "criterion = torch.nn.MSELoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03828f61",
   "metadata": {},
   "source": [
    "Once the model is defined, we extract the input size (i.e., the number of features) directly from the DataFrame to configure the input layer accordingly. After that, we set up three key components from PyTorch that are essential for training: the loss function, the optimizer, and the learning rate scheduler.\n",
    "\n",
    "criterion defines the loss function that measures how well the model performs. In our case, we use Mean Squared Error (MSE), which is commonly used for regression tasks.\n",
    "\n",
    "optimizer is the algorithm that adjusts the model’s weights to minimize the loss. We use a built-in optimizer from PyTorch (Adam), which updates the weights to improve temperature predictions over time.\n",
    "\n",
    "scheduler dynamically adjusts the learning rate during training. Since our model has a large number of parameters (e.g., weights and biases across layers of sizes 128, 64, and 32), the cost function’s surface is complex with many valleys. Initially, the optimizer takes larger steps to explore the space, but as it approaches a minimum, the scheduler reduces the learning rate (by a factor of 0.5 after 3 stagnant epochs in our case). This strategy helps the model converge more precisely to an optimal solution without overshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83fe024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.001):\n",
    "        self.patience = patience            \n",
    "        self.min_delta = min_delta          \n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0  # Se reinicia porque hay mejora\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "early_stopper = EarlyStopping(patience=10, min_delta=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b48e6",
   "metadata": {},
   "source": [
    "The final class we define is EarlyStopping, an essential component for training our neural network efficiently. Its purpose is to stop the training process if the model does not show meaningful improvement after a certain number of consecutive epochs. Specifically, if the validation loss does not improve by at least min_delta = 0.001 over 10 consecutive epochs, training will be halted. This helps prevent overfitting and saves computational resources by avoiding unnecessary training once the model has stopped learning and achieved a locally optimal configuration. \n",
    "\n",
    "Now we shall move onto the actual training process of our neural network and how we implemented it using PyTorch. Since this is the core of our project we shall explain it carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5481ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer):\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (x_batch, y_batch, weights_batch) in enumerate(train_loader):\n",
    "        pred = model(x_batch).squeeze()\n",
    "        losses = criterion(pred, y_batch)\n",
    "        loss = losses.sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c9c7bf",
   "metadata": {},
   "source": [
    "Above, we showed the training function. It receives the model (RedNeuronal(input_size)), the training loader (which contains the training dataset shuffled and split into batches of size 256), the loss criterion (MSE), and the optimizer (Adam).\n",
    "\n",
    "Each batch consists of 256 data samples, each containing features and targets. For every sample in the batch, the neural network predicts an output from the features, and then we calculate the loss using the MSE function by comparing the prediction with the actual observation. After processing all 256 samples in the batch, we compute the average loss, which represents the overall error for that batch. A higher loss means the model’s predictions were less accurate.\n",
    "\n",
    "Once the loss for the batch is calculated, the optimizer computes the gradient of this loss with respect to the model’s parameters and updates the weights to minimize the error, following the path of steepest descent.\n",
    "\n",
    "This entire process is repeated for all batches in the training loader during each epoch. By dividing the total loss accumulated over all batches by the number of batches, we obtain the average training loss for the epoch.\n",
    "\n",
    "The training continues for as many epochs as specified by the EarlyStopping function or until the maximum number of epochs is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be467a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, scaler_Y):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_celsius = []\n",
    "    targets_celsius = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            pred_val = model(x_val).squeeze()\n",
    "            loss = criterion(pred_val, y_val).mean()\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            pred_kelvin = scaler_Y.inverse_transform(pred_val.cpu().numpy().reshape(-1, 1))\n",
    "            target_kelvin = scaler_Y.inverse_transform(y_val.cpu().numpy().reshape(-1, 1))\n",
    "            preds_celsius.extend((pred_kelvin - 273.15).flatten())\n",
    "            targets_celsius.extend((target_kelvin - 273.15).flatten())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    MAE_celsius = np.mean(np.abs(np.array(preds_celsius) - np.array(targets_celsius)))\n",
    "\n",
    "    return avg_val_loss, MAE_celsius"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d3678d",
   "metadata": {},
   "source": [
    "Now we move on to the validation process. This step is crucial to determine whether the neural network is truly learning or just memorizing the training data. During validation, we turn off gradients because we only want to evaluate the model’s performance, not update its weights. For each data sample in the validation dataset, we compute the predicted values and then calculate the total loss. We obtain the average validation loss by dividing this total by the number of batches in the validation loader. To better understand how well the model is performing in real-world terms, we rescale the predicted and observed temperatures back to their original units and convert them to Celsius. This gives us a physical measure of error that is easier to interpret.\n",
    "\n",
    "We then calculate the Mean Absolute Error (MAE) in Celsius and print it alongside the average validation loss at each validation step. Since the dataset was initially rescaled, the validation loss itself does not carry a direct physical meaning, it only indicates whether the model’s performance is improving or worsening. That’s why the MAE in Celsius is important: it provides an approximate measure of the model’s expected error when forecasting the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06791460",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 60\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "    avg_val_loss, MAE_celsius = validate(model, val_loader, criterion, scaler_Y)\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_path = f\"./Modelos/{run_name}/{timestamp}/WRFTA_{run_suffix}.pt\"\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    # Scheduler checks the average loss\n",
    "    scheduler.step(avg_val_loss) \n",
    "    # Early stopper checks the average loss\n",
    "    early_stopper(avg_val_loss) \n",
    "\n",
    "    model.train()\n",
    "    # Printing the metrics for each epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss entrenamiento: {avg_loss:.4f}, Loss validación (escalado): {avg_val_loss:.4f}, MAE validación (°C): {MAE_celsius:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopper.early_stop:\n",
    "        print(f\"Parada temprana en la época {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Loading the best model\n",
    "print('Cargando el mejor modelo guardado...')\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "print('Comenzando la predicción...')\n",
    "model.eval()\n",
    "all_preds_scaled = []\n",
    "all_targets_scaled = []\n",
    "\n",
    "# We use the best model to make predictions on the entire dataset\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in loader:\n",
    "        y_pred = model(x_batch).squeeze()\n",
    "        all_preds_scaled.append(y_pred.numpy())\n",
    "        all_targets_scaled.append(y_batch.numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f16d19",
   "metadata": {},
   "source": [
    "Once the training and validation functions are defined, we can begin training our neural network. The process is straightforward: in each epoch, we train the model and validate its performance, saving the configuration that achieves the best average validation loss. Meanwhile, the scheduler and early stopper monitor the network’s progress, adjusting the learning rate or stopping the process entirely if necessary.\n",
    "\n",
    "After the training loop finishes (EarlyStopper summoned or maximum num_epochs achieved), we load the best saved configuration and use it to generate predictions for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a5d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_scaled = np.concatenate(all_preds_scaled).reshape(-1, 1)\n",
    "all_targets_scaled = np.concatenate(all_targets_scaled).reshape(-1, 1)\n",
    "\n",
    "# Desescalamos\n",
    "y_pred_real = scaler_Y.inverse_transform(all_preds_scaled).flatten()\n",
    "y_test_real = scaler_Y.inverse_transform(all_targets_scaled).flatten()\n",
    "\n",
    "# DataFrame con resultados completos\n",
    "df_resultados = pd.DataFrame({\n",
    "    'y_pred_C': y_pred_real - 273.15, # OJO y_pred_real es la predicción del modelo\n",
    "    'TA_C': y_test_real - 273.15 # y_test_real es la temperatura observada\n",
    "})\n",
    "\n",
    "print(f'Error medio absoluto (MAE): {(np.mean(np.abs(y_test_real - y_pred_real))):.4f}')\n",
    "\n",
    "print(f'ds.columns: {ds.columns}')\n",
    "print(f'ds.shape {ds.shape}')\n",
    "print(f'df_resultados.columns: {df_resultados.columns}')\n",
    "print(f'df_resultado.shape: {df_resultados.shape}')\n",
    "\n",
    "# Pegamos directamente las predicciones al dataframe original\n",
    "ds = ds.reset_index(drop=True)  # Aseguramos índices alineados\n",
    "\n",
    "# Añadimos la columna de predicciones al df original\n",
    "df = pd.concat([ds[['fecha']], df_resultados], axis=1)\n",
    "\n",
    "\n",
    "print('Inicializando Plot')\n",
    "df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "df['dia_fecha'] = df['fecha'].dt.normalize()\n",
    "df['mes'] = df['fecha'].dt.to_period('M')\n",
    "\n",
    "# =================================================================================== CÁLCULO DE EXTREMOS DIARIOS ===================================================================================================================================================\n",
    "\n",
    "# Agrupamos por día para obtener temperaturas extremas (máximas y mínimas del día)\n",
    "\n",
    "extremos_diarios = df.groupby('dia_fecha').agg({\n",
    "    'TA_C': ['min', 'max'],\n",
    "    'y_pred_C': ['min', 'max'],\n",
    "    'mes': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Renombrar columnas\n",
    "extremos_diarios.columns = ['dia_fecha', 'TA_min', 'TA_max', 'y_pred_min', 'y_pred_max', 'mes']\n",
    "\n",
    "# ================================================================================= CÁLCULO DE ERRORES EN EXTREMOS ====================================================================================================================================================\n",
    "\n",
    "extremos_diarios['diff_max'] = np.abs(extremos_diarios['TA_max'] - extremos_diarios['y_pred_max'])\n",
    "extremos_diarios['diff_min'] = np.abs(extremos_diarios['TA_min'] - extremos_diarios['y_pred_min'])\n",
    "\n",
    "# =========================================================================== PROMEDIO MENSUAL DE LAS DIFERENCIAS DE EXTREMOS ===================================================================================================================================\n",
    "\n",
    "df_diffs_mensuales = extremos_diarios.groupby('mes')[[\n",
    "    'diff_max', 'diff_min'\n",
    "]].mean().reset_index()\n",
    "\n",
    "# ================================================================================== MAE de Tmedia diaria por modelo ===================================================================================================================================================\n",
    "\n",
    "df_diario = df.groupby('dia_fecha').agg({ \n",
    "    'TA_C': 'mean',\n",
    "    'y_pred_C': 'mean' # CALCULAMOS LA MEDIA DE LOS VALORES DE (df = DATA_final.csv) AGRUPADOS POR DIA (T_media_diaria)\n",
    "}).reset_index()\n",
    "\n",
    "df_diario['mes'] = df_diario['dia_fecha'].dt.to_period('M')\n",
    "\n",
    "df_error_mensual = df_diario.groupby('mes').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'MAE_diaria': np.mean(np.abs(x['y_pred_C'] - x['TA_C'])),\n",
    "    })\n",
    "\n",
    ").reset_index()\n",
    "\n",
    "# ====================================================================================== UNIR RESULTADOS MENSUALES =============================================================================================0\n",
    "\n",
    "df_mensual = df_diffs_mensuales.merge(df_error_mensual, on='mes') # AGRUPAMOS (df_diffs_mensuales) CON (df_error_mensual) POR MES \n",
    "\n",
    "# ================================================================================================ PLOTEO ================================================================================================\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.plot(df_mensual['mes'].astype(str), df_mensual['diff_max'], label='MAE Tmáx', marker='s', color='red')\n",
    "plt.plot(df_mensual['mes'].astype(str), df_mensual['diff_min'], label='MAE Tmín', marker='s', color='blue')\n",
    "plt.plot(df_mensual['mes'].astype(str), df_mensual['MAE_diaria'], label='MAE Tmedia', marker='s', color='cyan')\n",
    "\n",
    "plt.axhline(0, linestyle='--', color='gray', linewidth=1)\n",
    "\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('Diferencia temperatura extrema (°C)')\n",
    "plt.title('Promedio mensual de la diferencia entre temperaturas extremas observadas y predichas')\n",
    "plt.legend(loc='best')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "############################################################################################### PRINTS VARIADOS ##################################################################################################\n",
    "\n",
    "mae_global = np.mean(np.abs(df['TA_C'] - df['y_pred_C']))\n",
    "\n",
    "print(f\"MAE {mae_global:.3f}\")\n",
    "mae_max = np.mean(df_mensual[\"diff_max\"])\n",
    "mae_min = np.mean(df_mensual[\"diff_min\"])\n",
    "print(f'MAE_máximas {mae_max}')\n",
    "print(f'MAE_mínimas {mae_min}')\n",
    "\n",
    "plt.savefig(f'{run_suffix}_RED_diferencias_extremos_diarios_promedio_mensual.png', dpi=300)\n",
    "\n",
    "################################################################################################### BOXPLOT #######################################################################################################3\n",
    "\n",
    "# Error Tmáx por mes\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(x=extremos_diarios['mes'].astype(str), y=extremos_diarios['diff_max'], color='red')\n",
    "plt.title('Distribución del error absoluto en Tmáx por mes')\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('Error absoluto (°C)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{run_suffix}_boxplot_Tmax.png', dpi=300)\n",
    "\n",
    "# Error Tmín por mes\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(x=extremos_diarios['mes'].astype(str), y=extremos_diarios['diff_min'], color='blue')\n",
    "plt.title('Distribución del error absoluto en Tmín por mes')\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('Error absoluto (°C)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{run_suffix}_boxplot_Tmin.png', dpi=300)\n",
    "\n",
    "# Error Tmedia diaria por mes\n",
    "df_diario['error_abs'] = np.abs(df_diario['TA_C'] - df_diario['y_pred_C'])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(x=df_diario['mes'].astype(str), y=df_diario['error_abs'], color='cyan')\n",
    "plt.title('Distribución del error absoluto en Tmedia diaria por mes')\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('Error absoluto (°C)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{run_suffix}_boxplot_Tmedia.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VENV",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
